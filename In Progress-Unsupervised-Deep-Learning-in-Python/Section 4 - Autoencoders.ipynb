{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4 - Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder is a supervised machine learning model trying to predict itself.\n",
    "With that process, encodings in the hidden layers create a different representation of the data.\n",
    "Generally, the loss function of choice is the squared error function, however, other error function can be used as well, like cross-entropy assuming the output being either 0 or 1.\n",
    "That is obtained by using the sigmoid function.\n",
    "\n",
    "### Shared Weights\n",
    "Share weights is a slight modification to normal approach where each layer has its own set of weights.\n",
    "In this scenario, with one hidden layers, the network can be described by following equations:\n",
    "\\begin{equation*}\n",
    "Z = s(X.dot(W) + b_{h})\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\check{X} = s(Z.dot(W^{T}) + b_{o} )\n",
    "\\end{equation*}\n",
    "\n",
    "This technique can be considered a regularization technique, since it reduced number of parameters.\n",
    "\n",
    "### Objective Function - Squared error\n",
    "\\begin{equation*}\n",
    "J = |X - \\check{X}|^2_{F} = |X - s(s(XW)W^{T})|^2_{F} \n",
    "\\end{equation*}\n",
    "\n",
    "That resembles PCA's objective function, which is given as:\n",
    "\\begin{equation*}\n",
    "J = |X - XQQ^{T}|^2_{F} \n",
    "\\end{equation*}\n",
    "\n",
    "Because of that autoencoders are like nonlinear PCA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# The data is stored in folder for Section 2\n",
    "# It will be re-used.\n",
    "def getKaggleMNIST():\n",
    "    # Column 0 is labels\n",
    "    # Column 1-785 is data, with values 0 .. 255\n",
    "    train = pd.read_csv('./data/Section 2/train.csv').as_matrix().astype(np.float32)\n",
    "    train = shuffle(train)\n",
    "\n",
    "    Xtrain = train[:-1000, 1:] / 255\n",
    "    Ytrain = train[:-1000, 0].astype(np.int32)\n",
    "\n",
    "    Xtest  = train[-1000:, 1:] / 255\n",
    "    Ytest  = train[-1000:, 0].astype(np.int32)\n",
    "    return Xtrain, Ytrain, Xtest, Ytest\n",
    "\n",
    "Xtrain, Ytrain, Xtest, Ytest = getKaggleMNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
