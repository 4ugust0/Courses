{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 - Principal Components Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA is a linear transformation: \n",
    "\n",
    "\\begin{equation*}\n",
    "Z = XQ\n",
    "\\end{equation*}\n",
    "\n",
    "If a vector is multiplied by a scalar, the direction stays the same, however, the length can be changes.\n",
    "\n",
    "If a vector is multiplies by a matrix it can \"point\" in a new direction (rotated).\n",
    "\n",
    "Input data X is of shape (N, D).\n",
    "\n",
    "Tranformation matrix Q is of shape (D x D).\n",
    "\n",
    "Transformed data Z is of shape (N x D).\n",
    "\n",
    "Multiplying a vector by a matrix can be seen as rotating the coordinate system of the vector.\n",
    "\n",
    "### PCA is often used for dimensionality reduction.\n",
    "\n",
    "Information carried by a variable can be assesed by measuring the variance.\n",
    "\n",
    "A variable having variance equal to zero provides no information.\n",
    "\n",
    "### Another advantage of PCA is that it de-correlate the variables.\n",
    "\n",
    "High correlation between two variables means that one of them can be disregarded since they provide (approximately) very similar information. \n",
    "\n",
    "The goal of PCA is to find a matrix Q that will help us find the correlation between the variable so that we have the smallest number of the most crucial information   \n",
    "\n",
    "### PCA can help reducing the impact of noise\n",
    "\n",
    "Assuming that the noise is not a major factor in the data, it is fair to state the principal component of the noise will be somewhere down the list. Thus, the PCA transformation will perform denoising pre-processing.\n",
    "\n",
    "Furthermore, by reducing number of parameters, risk of fitting the model to the noise or overfitting is significantly reduced.\n",
    "\n",
    "### Latent variables Z can be interpreted as the underlying cause of X\n",
    "\n",
    "That examplains them being significantly uncorrelated. \n",
    " \n",
    "Very important: Uncorrelated does not necessarily mean independent, unless distribution is Gaussian.\n",
    "\n",
    "### In PCA, the linear transformation can be applied both ways:\n",
    "\n",
    "\\begin{equation*}\n",
    "Z = XQ\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "X = ZQ^{-1}\n",
    "\\end{equation*}\n",
    "\n",
    "### PCA Derivation\n",
    "\n",
    "Covariance of X is given as:\n",
    "\\begin{equation*}\n",
    "Cov(X)= \\frac{1}{N} (X - \\mu _{X})^{T} (X - \\mu _{X})\n",
    "\\end{equation*}\n",
    "\n",
    "Cov(X) is a D by D matrix.\n",
    "\n",
    "\n",
    "### Eigenvalues and Eigenvectors\n",
    "\n",
    "Eigenvectors (v) are non-zero vectors vectors  that only change by a scalar factor , when a linear transformation is applied to it.\n",
    "\n",
    "\\begin{equation*}\n",
    "Av = \\lambda v\n",
    "\\end{equation*}\n",
    "\n",
    "The equation can be re-written as:\n",
    "\n",
    "\\begin{equation*}\n",
    "| Av - \\lambda I | v = 0\n",
    "\\end{equation*}\n",
    "\n",
    "It has a non-zero solution if and only if the determinant is zero:\n",
    "\\begin{equation*}\n",
    "det| Av - \\lambda I | = 0\n",
    "\\end{equation*}\n",
    "\n",
    "&#955; is known as the eigenvalues or characteristic root associated with the eigenvector v.\n",
    "\n",
    "The goal is to find the eigenvalues and corresponding to them eigenvectors.\n",
    "\n",
    "There will be D number of eigenvalues (because X is of shape N by D) and their values will be greater or equal to zero.\n",
    "\n",
    "After finding the eigenvectors and corresponding to them eigenvalues the eigenvalues will be sorted in the descending order and combined in a diagonal matrix:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\wedge = \\begin{pmatrix}\n",
    "\\lambda_{1} & 0 & . & 0\\\\ \n",
    "0 & \\lambda_{2} & . & 0 \\\\ \n",
    ". & . & . &. \\\\ \n",
    ". & . & . & \\lambda_{D}\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "A V matrix is constructed as follows:\n",
    "\\begin{equation*}\n",
    "V = \\begin{pmatrix}\n",
    "1 & 1  & .  & 1 \\\\ \n",
    "v_{1} & v_{2} & . & v_{D}\\\\ \n",
    "1 & 1 & . &  1\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "The V matrix is orthonormal which results in that if the vector V is multiplied by another eigenvector the result will be zero.\n",
    "\n",
    "\\begin{equation*}\n",
    "Cov(X)V = V \\lambda\n",
    "\\end{equation*}\n",
    "\n",
    "Furthermore, covariance of Z is given as:\n",
    "\\begin{equation*}\n",
    "Cov(Z)= \\frac{1}{N} (Z - \\mu _{Z})^{T} (Z - \\mu _{Z}) = \\frac{1}{N} (XQ - \\mu _{X}Q)^{T} (XQ - \\mu _{X}Q) =  \\frac{1}{N} Q^{T}(X - \\mu _{X})^{T} (X - \\mu _{X})Q = Q^{T} Cov(X) Q\n",
    "\\end{equation*}\n",
    "\n",
    "By choosing Q = V we obtain:\n",
    "\\begin{equation*}\n",
    "Cov(Z) = V^{T} Cov(X) V = V^{T} V \\wedge = \\wedge\n",
    "\\end{equation*}\n",
    "\n",
    "Off diagonal elements are equal to zero which means that no dimension is correlated with any other dimension.\n",
    "All values of &#955; were sorted in descending order so the first column of Z has the most variance and thus carries the most information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
