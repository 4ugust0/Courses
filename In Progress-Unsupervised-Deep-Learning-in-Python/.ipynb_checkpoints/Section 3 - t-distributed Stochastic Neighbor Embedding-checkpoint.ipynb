{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3 - t-distributed Stochastic Neighbor Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE\n",
    "- t-SNE is a nonlinear algorithms developed by Laurens van der Maaten and Geoffrey Hinton.\n",
    "\n",
    "- The name of the algorithm comes from its incorporation of the \"t\" distribution and stochastic neighbour embedding.\n",
    "\n",
    "- t-SNE does not provide any transformation model, because it modifies the outputs directly to minimize the cost function.\n",
    "No other data can be transformed but that which the algorithm was just fitted to.\n",
    "\n",
    "### Basic concept - symmetric SNE\n",
    "t-SNE tries to preserve distances between each input vector.\n",
    "\n",
    "1) Create a probability distribution p(i, j) where sigma is a hyperparameter:\n",
    "\n",
    "\\begin{equation*}\n",
    "p_{ij} = \\frac{\\exp\\left(-\\left || x_i - x_j\\right | |^2 \\big/ 2\\sigma^2\\right)}{\\displaystyle\\sum_{k \\neq l} \\exp\\left(-\\left|| x_k - x_l\\right||^2 \\big/ 2\\sigma^2\\right)}\n",
    "\\end{equation*}\n",
    "\n",
    "2) Initialize randomly low-dimensional mapping Y (N by k vector, where k << D) and define q(i,j) as: \n",
    "\n",
    "\\begin{equation*}\n",
    "q_{ij} = \\frac{\\exp\\left(-\\left || y_i - y_j\\right | |^2 \\right)}{\\displaystyle\\sum_{k \\neq l} \\exp\\left(-\\left|| y_k - y_l\\right||^2 \\right)}\n",
    "\\end{equation*}\n",
    "\n",
    "With symetric SNE, it is not important how far something is from itself thus:\n",
    "\\begin{equation*}\n",
    "p_{ij}  = q_{ij} = 0\n",
    "\\end{equation*}\n",
    "\n",
    "The goal is to obtain p(i, j) as close to q(i, j) as possible.\n",
    "\n",
    "In order to compare two probability distributions, we can use Kullback-Leibler (KL) divergence:\n",
    "\\begin{equation*}\n",
    "C  = KL(P || Q) = \\sum_{i} \\sum_{j} p_{ij} log \\frac{p_{ij}}{q_{ij}}\n",
    "\\end{equation*}\n",
    "\n",
    "An apptoximated solution is obtained by calculating the derivative of the cost function C and gradient descent:\n",
    "\\begin{equation*}\n",
    "\\frac{\\delta C}{\\delta y_{i}} = 4 \\sum_{j} (p_{ij}-q_{ij})(y_{i} - y_{j})\n",
    "\\end{equation*}\n",
    "\n",
    "This model has no weights, so the update affects the mapping directly.\n",
    "\n",
    "Symmetric SNE suffers from crowding problem.\n",
    "\n",
    "### t-SNE \n",
    "t-SNE uses t-distribution for the q mapping:\n",
    "\n",
    "\\begin{equation*}\n",
    "q_{ij} = \\frac{\\left(1+\\left|| y_i - y_j\\right||^2 \\right)^{-1}}{\\displaystyle\\sum_{k \\neq i} \\left(1+\\left|| y_i - y_j\\right||^2 \\right)^{-1}}\n",
    "\\end{equation*}\n",
    "\n",
    "And a different p:\n",
    "\\begin{equation*}\n",
    "p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2n}\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "p_{j|i} = \\frac{\\exp\\left(-\\left| x_i - x_j\\right|^2 \\big/ 2\\sigma_i^2\\right)}{\\displaystyle\\sum_{k \\neq i} \\exp\\left(-\\left| x_i - x_k\\right|^2 \\big/ 2\\sigma_i^2\\right)}\n",
    "\\end{equation*}\n",
    "\n",
    "### t-SNE limitations\n",
    "- Huge RAM requirements\n",
    "- We need to calculate q(i,j) and p(i,j) for i in range (1, N) and j in range (1, N) thus the algorithm grows N-squared times\n",
    "- Default Ski-kit learn method Barnes-Hut runs in O(NlonN) time which is better, but still requires a lot of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
