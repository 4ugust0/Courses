{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Section 1 - Introduction and Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Notes\n",
    "- Reinforcement learning is more different from supervised and unsupervised machine learning are from each other.\n",
    "- Both supervised and unsupervised machine learning use gathered, labelled or unlabelled data.\n",
    "- Reinforcement learning interfaces with an environment (simulated environment or through sensors).\n",
    "- Objective of the RL is to minimize the cost / maximize the reward.\n",
    "- The agent makes actions and feedback signals (in form of the reward) are automatically given to the agent by the environment.\n",
    "- This course will only look at finite state space enviroment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tic-tac-toe example\n",
    "1) What is the number of states?\n",
    "- The board has 9 locations.\n",
    "- Each location on the board has 3 possibilities: empty, X and O.\n",
    "- The example is simplified but not finishing the game when a player wins. \n",
    "- With that the game has following number of states: $$ 3x3x3x3x3x3x3x3x3 = 3^9 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Important vocabulary\n",
    "- Agent: goal of the development, it interacts with the environment. Hopefully with an intelligent manner.\n",
    "- Environment: real or simulated world providing sensory data to the agent in for states, list of actions and a reward.\n",
    "- State: a configuration of the environment sensed by the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Notes\n",
    "- The agent tried to maximize its immediate reward, but future rewards as well\n",
    "- The reward must be programmer intelligently to avoid unintended consequences.\n",
    "- The reward as always a real number.\n",
    "- SAR = State, Action, Reward\n",
    "- Every game is a sequence of states, actions and rewards\n",
    "- Convention: start in state S(t), take action A(t), receive a reward of R(t+1)\n",
    "- R(t+1) is always a result of the agent making action A(t) as state S(t)\n",
    "- S(t) and A(t) result in environment changing to state S(t+1)\n",
    "- Triple [S(t), A(t), S(t+1)] is denoted as (s, a, s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Section 2 - Return of the Multi-Armed Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
