{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8.5 - Introduction to generative adversarial networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dense, LeakyReLU, Input, Conv2DTranspose, Reshape, Conv2D\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latent_dim = 32\n",
    "height = 32\n",
    "width = 32\n",
    "channels = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32768)             1081344   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 256)       819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 32, 32, 256)       1048832   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 3)         37635     \n",
      "=================================================================\n",
      "Total params: 6,264,579\n",
      "Trainable params: 6,264,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_input = Input(shape=(latent_dim,))\n",
    "\n",
    "# First, transform the input into a 16x16 128-channels feature map\n",
    "x = Dense(units = 128 * 16 * 16)(generator_input)\n",
    "x = LeakyReLU()(x)\n",
    "x = Reshape(target_shape = (16, 16, 128))(x)\n",
    "\n",
    "# Then, add a convolution layer\n",
    "x = Conv2D(filters = 256, \n",
    "           kernel_size = (5, 5), \n",
    "           padding = 'same')(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "# Upsample to 32x32\n",
    "x = Conv2DTranspose(filters = 256, \n",
    "                    kernel_size = (4, 4), \n",
    "                    strides = (2, 2), \n",
    "                    padding = 'same')(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "# Few more conv layers\n",
    "x = Conv2D(filters = 256, \n",
    "           kernel_size = (5, 5),  \n",
    "           padding = 'same')(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Conv2D(filters = 256, \n",
    "           kernel_size = (5, 5), \n",
    "           padding = 'same')(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "# Produce a 32x32 1-channel feature map\n",
    "x = Conv2D(filters = channels, \n",
    "           kernel_size = (7, 7),\n",
    "           activation = 'tanh', \n",
    "           padding = 'same')(x)\n",
    "generator = Model(generator_input, x)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Flatten, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 30, 30, 128)       3584      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 2, 2, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 790,913\n",
      "Trainable params: 790,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator_input = Input(shape = (height, width, channels))\n",
    "x = Conv2D(filters = 128,  \n",
    "           kernel_size = (3, 3))(discriminator_input)\n",
    "x = LeakyReLU()(x)\n",
    "x = Conv2D(filters = 128,  \n",
    "           kernel_size = (4, 4), \n",
    "           strides = (2, 2))(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Conv2D(filters = 128,  \n",
    "           kernel_size = (4, 4), \n",
    "           strides = (2, 2))(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Conv2D(filters = 128,  \n",
    "           kernel_size = (4, 4), \n",
    "           strides = (2, 2))(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "# One dropout layer - important trick!\n",
    "x = Dropout(rate = 0.4)(x)\n",
    "\n",
    "# Classification layer\n",
    "x = Dense(units = 1, \n",
    "          activation = 'sigmoid')(x)\n",
    "\n",
    "discriminator = Model(discriminator_input, x)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To stabilize training, we use learning rate decay\n",
    "# and gradient clipping (by value) in the optimizer.\n",
    "discriminator_optimizer = RMSprop(lr = 0.0008, \n",
    "                                  clipvalue = 1.0, \n",
    "                                  decay = 1e-8)\n",
    "discriminator.compile(optimizer = discriminator_optimizer, \n",
    "                      loss = 'binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The adversarial network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set discriminator weights to non-trainable\n",
    "# (will only apply to the `gan` model)\n",
    "discriminator.trainable = False\n",
    "\n",
    "gan_input = Input(shape = (latent_dim,))\n",
    "gan_output = discriminator(generator(gan_input))\n",
    "gan = Model(gan_input, gan_output)\n",
    "\n",
    "gan_optimizer = RMSprop(lr = 0.0004, \n",
    "                        clipvalue = 1.0, \n",
    "                        decay = 1e-8)\n",
    "gan.compile(optimizer = gan_optimizer, \n",
    "            loss = 'binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.preprocessing import image\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load CIFAR10 data\n",
    "(x_train, y_train), (_, _) = cifar10.load_data()\n",
    "\n",
    "# Select images (class 5)\n",
    "x_train = x_train[y_train.flatten() == 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 32, 32, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "x_train = x_train.reshape((x_train.shape[0],) + (height, width, channels)).astype('float32') / 255.\n",
    "\n",
    "iterations = 10000\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 32, 32, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_dir = './data/Chapter 8.5 - Introduction to generative adversarial networks/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:953: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "--------------------------------------------------\n",
      "Step: 0\n",
      "Discriminator loss: 0.6911715\n",
      "Adversarial loss: 0.6933484\n",
      "--------------------------------------------------\n",
      "Step: 100\n",
      "Discriminator loss: 0.44683647\n",
      "Adversarial loss: 2.628831\n",
      "--------------------------------------------------\n",
      "Step: 200\n",
      "Discriminator loss: 0.70925105\n",
      "Adversarial loss: 0.8020114\n",
      "--------------------------------------------------\n",
      "Step: 300\n",
      "Discriminator loss: 0.7145446\n",
      "Adversarial loss: 0.77895004\n",
      "--------------------------------------------------\n",
      "Step: 400\n",
      "Discriminator loss: 0.7032187\n",
      "Adversarial loss: 0.7404046\n",
      "--------------------------------------------------\n",
      "Step: 500\n",
      "Discriminator loss: 0.69979143\n",
      "Adversarial loss: 0.7711795\n",
      "--------------------------------------------------\n",
      "Step: 600\n",
      "Discriminator loss: 0.6790079\n",
      "Adversarial loss: 1.15787\n",
      "--------------------------------------------------\n",
      "Step: 700\n",
      "Discriminator loss: 0.69431365\n",
      "Adversarial loss: 0.7624009\n",
      "--------------------------------------------------\n",
      "Step: 800\n",
      "Discriminator loss: 0.75251454\n",
      "Adversarial loss: 0.90363896\n",
      "--------------------------------------------------\n",
      "Step: 900\n",
      "Discriminator loss: 0.6952241\n",
      "Adversarial loss: 0.8307866\n",
      "--------------------------------------------------\n",
      "Step: 1000\n",
      "Discriminator loss: 0.7054943\n",
      "Adversarial loss: 0.7260996\n",
      "--------------------------------------------------\n",
      "Step: 1100\n",
      "Discriminator loss: 0.70006144\n",
      "Adversarial loss: 0.7457956\n",
      "--------------------------------------------------\n",
      "Step: 1200\n",
      "Discriminator loss: 0.6897727\n",
      "Adversarial loss: 0.7516589\n",
      "--------------------------------------------------\n",
      "Step: 1300\n",
      "Discriminator loss: 0.69468915\n",
      "Adversarial loss: 0.7897028\n",
      "--------------------------------------------------\n",
      "Step: 1400\n",
      "Discriminator loss: 0.69670707\n",
      "Adversarial loss: 0.73326683\n",
      "--------------------------------------------------\n",
      "Step: 1500\n",
      "Discriminator loss: 0.68500197\n",
      "Adversarial loss: 0.68625075\n",
      "--------------------------------------------------\n",
      "Step: 1600\n",
      "Discriminator loss: 0.67639923\n",
      "Adversarial loss: 0.80879366\n",
      "--------------------------------------------------\n",
      "Step: 1700\n",
      "Discriminator loss: 0.7212465\n",
      "Adversarial loss: 0.7429582\n",
      "--------------------------------------------------\n",
      "Step: 1800\n",
      "Discriminator loss: 0.6789199\n",
      "Adversarial loss: 0.7903185\n",
      "--------------------------------------------------\n",
      "Step: 1900\n",
      "Discriminator loss: 0.6723213\n",
      "Adversarial loss: 0.8794702\n",
      "--------------------------------------------------\n",
      "Step: 2000\n",
      "Discriminator loss: 0.6927999\n",
      "Adversarial loss: 0.87353134\n",
      "--------------------------------------------------\n",
      "Step: 2100\n",
      "Discriminator loss: 0.7029891\n",
      "Adversarial loss: 0.7775944\n",
      "--------------------------------------------------\n",
      "Step: 2200\n",
      "Discriminator loss: 0.68108535\n",
      "Adversarial loss: 0.90648127\n",
      "--------------------------------------------------\n",
      "Step: 2300\n",
      "Discriminator loss: 0.69884574\n",
      "Adversarial loss: 0.77626145\n",
      "--------------------------------------------------\n",
      "Step: 2400\n",
      "Discriminator loss: 0.6791872\n",
      "Adversarial loss: 0.77388597\n",
      "--------------------------------------------------\n",
      "Step: 2500\n",
      "Discriminator loss: 0.69965726\n",
      "Adversarial loss: 0.67664176\n",
      "--------------------------------------------------\n",
      "Step: 2600\n",
      "Discriminator loss: 0.66865647\n",
      "Adversarial loss: 0.85385674\n",
      "--------------------------------------------------\n",
      "Step: 2700\n",
      "Discriminator loss: 0.6744262\n",
      "Adversarial loss: 0.97089624\n",
      "--------------------------------------------------\n",
      "Step: 2800\n",
      "Discriminator loss: 0.68916875\n",
      "Adversarial loss: 0.77362955\n",
      "--------------------------------------------------\n",
      "Step: 2900\n",
      "Discriminator loss: 0.70223516\n",
      "Adversarial loss: 0.7849681\n",
      "--------------------------------------------------\n",
      "Step: 3000\n",
      "Discriminator loss: 0.6773055\n",
      "Adversarial loss: 0.8516523\n",
      "--------------------------------------------------\n",
      "Step: 3100\n",
      "Discriminator loss: 0.6643353\n",
      "Adversarial loss: 0.86221564\n",
      "--------------------------------------------------\n",
      "Step: 3200\n",
      "Discriminator loss: 0.6684871\n",
      "Adversarial loss: 0.8877943\n",
      "--------------------------------------------------\n",
      "Step: 3300\n",
      "Discriminator loss: 0.6550965\n",
      "Adversarial loss: 0.86263144\n",
      "--------------------------------------------------\n",
      "Step: 3400\n",
      "Discriminator loss: 0.70162416\n",
      "Adversarial loss: 0.6676649\n",
      "--------------------------------------------------\n",
      "Step: 3500\n",
      "Discriminator loss: 0.6952876\n",
      "Adversarial loss: 0.99171543\n",
      "--------------------------------------------------\n",
      "Step: 3600\n",
      "Discriminator loss: 0.6895224\n",
      "Adversarial loss: 0.9264434\n",
      "--------------------------------------------------\n",
      "Step: 3700\n",
      "Discriminator loss: 0.71681684\n",
      "Adversarial loss: 0.8614456\n",
      "--------------------------------------------------\n",
      "Step: 3800\n",
      "Discriminator loss: 0.6729911\n",
      "Adversarial loss: 1.0582005\n",
      "--------------------------------------------------\n",
      "Step: 3900\n",
      "Discriminator loss: 0.6995832\n",
      "Adversarial loss: 0.81170386\n",
      "--------------------------------------------------\n",
      "Step: 4000\n",
      "Discriminator loss: 0.69235384\n",
      "Adversarial loss: 1.0145327\n",
      "--------------------------------------------------\n",
      "Step: 4100\n",
      "Discriminator loss: 0.7924156\n",
      "Adversarial loss: 2.4178765\n",
      "--------------------------------------------------\n",
      "Step: 4200\n",
      "Discriminator loss: 0.7185844\n",
      "Adversarial loss: 0.7813747\n",
      "--------------------------------------------------\n",
      "Step: 4300\n",
      "Discriminator loss: 0.7019522\n",
      "Adversarial loss: 0.7702726\n",
      "--------------------------------------------------\n",
      "Step: 4400\n",
      "Discriminator loss: 0.68162084\n",
      "Adversarial loss: 0.82279253\n",
      "--------------------------------------------------\n",
      "Step: 4500\n",
      "Discriminator loss: 0.68529516\n",
      "Adversarial loss: 0.7544757\n",
      "--------------------------------------------------\n",
      "Step: 4600\n",
      "Discriminator loss: 0.7233573\n",
      "Adversarial loss: 0.8187416\n",
      "--------------------------------------------------\n",
      "Step: 4700\n",
      "Discriminator loss: 0.6822995\n",
      "Adversarial loss: 0.7368448\n",
      "--------------------------------------------------\n",
      "Step: 4800\n",
      "Discriminator loss: 0.68382776\n",
      "Adversarial loss: 0.7693602\n",
      "--------------------------------------------------\n",
      "Step: 4900\n",
      "Discriminator loss: 0.6784229\n",
      "Adversarial loss: 0.76925075\n",
      "--------------------------------------------------\n",
      "Step: 5000\n",
      "Discriminator loss: 0.6867162\n",
      "Adversarial loss: 0.8713625\n",
      "--------------------------------------------------\n",
      "Step: 5100\n",
      "Discriminator loss: 0.68624526\n",
      "Adversarial loss: 0.8143288\n",
      "--------------------------------------------------\n",
      "Step: 5200\n",
      "Discriminator loss: 0.68612945\n",
      "Adversarial loss: 0.82269776\n",
      "--------------------------------------------------\n",
      "Step: 5300\n",
      "Discriminator loss: 0.69660705\n",
      "Adversarial loss: 0.7579215\n",
      "--------------------------------------------------\n",
      "Step: 5400\n",
      "Discriminator loss: 0.6789893\n",
      "Adversarial loss: 0.8200084\n",
      "--------------------------------------------------\n",
      "Step: 5500\n",
      "Discriminator loss: 0.72082186\n",
      "Adversarial loss: 0.9304228\n",
      "--------------------------------------------------\n",
      "Step: 5600\n",
      "Discriminator loss: 0.7045619\n",
      "Adversarial loss: 0.79948837\n",
      "--------------------------------------------------\n",
      "Step: 5700\n",
      "Discriminator loss: 0.69796574\n",
      "Adversarial loss: 0.79910123\n",
      "--------------------------------------------------\n",
      "Step: 5800\n",
      "Discriminator loss: 0.68886065\n",
      "Adversarial loss: 0.7728654\n",
      "--------------------------------------------------\n",
      "Step: 5900\n",
      "Discriminator loss: 0.6807909\n",
      "Adversarial loss: 0.7921885\n",
      "--------------------------------------------------\n",
      "Step: 6000\n",
      "Discriminator loss: 0.68425834\n",
      "Adversarial loss: 0.7994927\n",
      "--------------------------------------------------\n",
      "Step: 6100\n",
      "Discriminator loss: 0.68168545\n",
      "Adversarial loss: 0.8104867\n",
      "--------------------------------------------------\n",
      "Step: 6200\n",
      "Discriminator loss: 0.69464415\n",
      "Adversarial loss: 0.72118616\n",
      "--------------------------------------------------\n",
      "Step: 6300\n",
      "Discriminator loss: 0.70137316\n",
      "Adversarial loss: 0.7575309\n",
      "--------------------------------------------------\n",
      "Step: 6400\n",
      "Discriminator loss: 0.6875243\n",
      "Adversarial loss: 0.8361305\n",
      "--------------------------------------------------\n",
      "Step: 6500\n",
      "Discriminator loss: 0.71526825\n",
      "Adversarial loss: 0.82820815\n",
      "--------------------------------------------------\n",
      "Step: 6600\n",
      "Discriminator loss: 0.68705666\n",
      "Adversarial loss: 0.7904312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Step: 6700\n",
      "Discriminator loss: 0.71096146\n",
      "Adversarial loss: 0.7600935\n",
      "--------------------------------------------------\n",
      "Step: 6800\n",
      "Discriminator loss: 0.6972765\n",
      "Adversarial loss: 0.7767426\n",
      "--------------------------------------------------\n",
      "Step: 6900\n",
      "Discriminator loss: 0.68509376\n",
      "Adversarial loss: 0.8056838\n",
      "--------------------------------------------------\n",
      "Step: 7000\n",
      "Discriminator loss: 0.7230104\n",
      "Adversarial loss: 0.7839001\n",
      "--------------------------------------------------\n",
      "Step: 7100\n",
      "Discriminator loss: 0.6922885\n",
      "Adversarial loss: 0.75965863\n",
      "--------------------------------------------------\n",
      "Step: 7200\n",
      "Discriminator loss: 0.6749347\n",
      "Adversarial loss: 0.6775611\n",
      "--------------------------------------------------\n",
      "Step: 7300\n",
      "Discriminator loss: 0.7141993\n",
      "Adversarial loss: 0.7120148\n",
      "--------------------------------------------------\n",
      "Step: 7400\n",
      "Discriminator loss: 0.68293697\n",
      "Adversarial loss: 0.7780585\n",
      "--------------------------------------------------\n",
      "Step: 7500\n",
      "Discriminator loss: 0.69306594\n",
      "Adversarial loss: 0.7886464\n",
      "--------------------------------------------------\n",
      "Step: 7600\n",
      "Discriminator loss: 0.7205754\n",
      "Adversarial loss: 0.73802483\n",
      "--------------------------------------------------\n",
      "Step: 7700\n",
      "Discriminator loss: 0.7114936\n",
      "Adversarial loss: 0.5833697\n",
      "--------------------------------------------------\n",
      "Step: 7800\n",
      "Discriminator loss: 0.7028869\n",
      "Adversarial loss: 0.6880201\n",
      "--------------------------------------------------\n",
      "Step: 7900\n",
      "Discriminator loss: 0.68306845\n",
      "Adversarial loss: 0.76671004\n",
      "--------------------------------------------------\n",
      "Step: 8000\n",
      "Discriminator loss: 0.686694\n",
      "Adversarial loss: 0.82968616\n",
      "--------------------------------------------------\n",
      "Step: 8100\n",
      "Discriminator loss: 0.6776139\n",
      "Adversarial loss: 0.7630439\n",
      "--------------------------------------------------\n",
      "Step: 8200\n",
      "Discriminator loss: 0.681163\n",
      "Adversarial loss: 0.7413414\n",
      "--------------------------------------------------\n",
      "Step: 8300\n",
      "Discriminator loss: 0.6898142\n",
      "Adversarial loss: 0.8415512\n",
      "--------------------------------------------------\n",
      "Step: 8400\n",
      "Discriminator loss: 0.6928853\n",
      "Adversarial loss: 0.7913861\n",
      "--------------------------------------------------\n",
      "Step: 8500\n",
      "Discriminator loss: 0.68659353\n",
      "Adversarial loss: 0.7492336\n",
      "--------------------------------------------------\n",
      "Step: 8600\n",
      "Discriminator loss: 0.68708456\n",
      "Adversarial loss: 0.77956426\n",
      "--------------------------------------------------\n",
      "Step: 8700\n",
      "Discriminator loss: 0.697896\n",
      "Adversarial loss: 0.7403568\n",
      "--------------------------------------------------\n",
      "Step: 8800\n",
      "Discriminator loss: 0.68905747\n",
      "Adversarial loss: 0.7231371\n",
      "--------------------------------------------------\n",
      "Step: 8900\n",
      "Discriminator loss: 0.70890033\n",
      "Adversarial loss: 0.81286156\n",
      "--------------------------------------------------\n",
      "Step: 9000\n",
      "Discriminator loss: 0.72729605\n",
      "Adversarial loss: 0.81993073\n",
      "--------------------------------------------------\n",
      "Step: 9100\n",
      "Discriminator loss: 0.6967311\n",
      "Adversarial loss: 0.83500254\n",
      "--------------------------------------------------\n",
      "Step: 9200\n",
      "Discriminator loss: 0.7148731\n",
      "Adversarial loss: 0.6921493\n",
      "--------------------------------------------------\n",
      "Step: 9300\n",
      "Discriminator loss: 0.6893544\n",
      "Adversarial loss: 0.7566522\n",
      "--------------------------------------------------\n",
      "Step: 9400\n",
      "Discriminator loss: 0.6894201\n",
      "Adversarial loss: 0.82959604\n",
      "--------------------------------------------------\n",
      "Step: 9500\n",
      "Discriminator loss: 0.69895107\n",
      "Adversarial loss: 0.9418046\n",
      "--------------------------------------------------\n",
      "Step: 9600\n",
      "Discriminator loss: 0.68523425\n",
      "Adversarial loss: 0.70057625\n",
      "--------------------------------------------------\n",
      "Step: 9700\n",
      "Discriminator loss: 0.68753856\n",
      "Adversarial loss: 0.84994173\n",
      "--------------------------------------------------\n",
      "Step: 9800\n",
      "Discriminator loss: 0.6732129\n",
      "Adversarial loss: 0.7845704\n",
      "--------------------------------------------------\n",
      "Step: 9900\n",
      "Discriminator loss: 0.7045146\n",
      "Adversarial loss: 0.7784\n"
     ]
    }
   ],
   "source": [
    "# Start training loop\n",
    "start = 0\n",
    "for step in range(iterations):\n",
    "    # Sample random points in the latent space\n",
    "    random_latent_vectors = np.random.normal(size = (batch_size, latent_dim))\n",
    "\n",
    "    # Decode them to fake images\n",
    "    generated_images = generator.predict(random_latent_vectors)\n",
    "\n",
    "    # Combine them with real images\n",
    "    stop = start + batch_size\n",
    "    real_images = x_train[start: stop]\n",
    "    combined_images = np.concatenate([generated_images, real_images])\n",
    "\n",
    "    # Assemble labels discriminating real from fake images\n",
    "    labels = np.concatenate([np.ones((batch_size, 1)),\n",
    "                             np.zeros((batch_size, 1))])\n",
    "    # Add random noise to the labels - important trick!\n",
    "    labels = labels + 0.05 * np.random.random(labels.shape)\n",
    "\n",
    "    # Train the discriminator\n",
    "    d_loss = discriminator.train_on_batch(combined_images, labels)\n",
    "\n",
    "    # Sample random points in the latent space\n",
    "    random_latent_vectors = np.random.normal(size = (batch_size, latent_dim))\n",
    "\n",
    "    # Assemble labels that say \"all real images\"\n",
    "    misleading_targets = np.zeros((batch_size, 1))\n",
    "\n",
    "    # Train the generator (via the gan model,\n",
    "    # where the discriminator weights are frozen)\n",
    "    a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n",
    "    \n",
    "    start = start + batch_size\n",
    "    if start > len(x_train) - batch_size:\n",
    "        start = 0\n",
    "\n",
    "    # Occasionally save / plot\n",
    "    if step % 100 == 0:\n",
    "        # Save model weights\n",
    "        gan.save_weights('./saved_checkpoints/Chapter 8.5 - Introduction to generative adversarial networks/gan.h5')\n",
    "\n",
    "        # Print metrics\n",
    "        print('-' * 50)\n",
    "        print('Step: %s' % step )\n",
    "        print('Discriminator loss: %s' % d_loss)\n",
    "        print('Adversarial loss: %s' % a_loss)\n",
    "\n",
    "        # Save one generated image\n",
    "        img = image.array_to_img(generated_images[0] * 255., \n",
    "                                 scale = False)\n",
    "        img.save(os.path.join(save_dir, 'generated_dog' + str(step) + '.png'))\n",
    "\n",
    "        # Save one real image, for comparison\n",
    "        img = image.array_to_img(real_images[0] * 255., \n",
    "                                 scale = False)\n",
    "        img.save(os.path.join(save_dir, 'real_dog' + str(step) + '.png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
